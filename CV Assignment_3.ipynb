{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b0ea57",
   "metadata": {},
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?\n",
    "   - Doubling the number of filters after each stride-2 convolution is a common practice in deep learning architectures, especially in convolutional neural networks (CNNs). The primary reason for this is to increase the network's capacity to capture and learn hierarchical features. As the spatial dimensions of the feature maps are reduced by half after a stride-2 convolution, the depth (number of filters) is increased to allow the network to capture more diverse and complex patterns. This helps the network learn features at different scales, from local details to global patterns.\n",
    "\n",
    "2. Why do we use a larger kernel with MNIST (with simple CNN) in the first conv?\n",
    "   - Using a larger kernel in the first convolutional layer of a CNN for MNIST is beneficial because it helps capture more global and high-level features from the input images. MNIST images are relatively small (28x28 pixels) and contain digits that are well-defined and occupy a significant portion of the image. A larger kernel (e.g., 5x5 or 7x7) allows the network to consider a broader context and learn features that represent the overall structure of the digits. This can improve the network's ability to recognize digits with various writing styles and orientations.\n",
    "\n",
    "3. What data is saved by ActivationStats for each layer?\n",
    "   - ActivationStats is a tool used for analyzing and visualizing the activations (output values) of each layer during training. For each layer, ActivationStats typically saves the following data:\n",
    "     - Activation Histograms: Histograms of activation values for each channel or neuron in the layer.\n",
    "     - Activation Statistics: Summary statistics such as mean, standard deviation, minimum, and maximum activation values.\n",
    "     - Activation Distributions: Visual representations of activation distributions.\n",
    "     - Activation Gradients: Information about gradients with respect to activations.\n",
    "   - This data helps researchers and practitioners understand how activations change during training and diagnose issues related to vanishing gradients, exploding gradients, and activation distributions.\n",
    "\n",
    "4. How do we get a learner's callback after they've completed training?\n",
    "   - To get a learner's callback after they've completed training, you can use callback functions provided by deep learning frameworks like PyTorch or TensorFlow. In PyTorch, for example, you can use the `on_train_end` method of a custom callback class. This method is called after the completion of training, allowing you to perform actions or computations based on the trained model and training history.\n",
    "\n",
    "5. What are the drawbacks of activations above zero?\n",
    "   - Activations above zero in neural networks can have some drawbacks, including:\n",
    "     - Vanishing Gradients: When activations saturate at high values, gradients during backpropagation can become very small, leading to slow convergence or gradient vanishing problems.\n",
    "     - Exploding Gradients: In some cases, very high activations can lead to exploding gradients, causing instability during training.\n",
    "     - Loss of Information: Extremely high activations may result in a loss of information as non-linearities like ReLU saturate and clip values above a certain threshold.\n",
    "\n",
    "6. Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "   - Benefits of Larger Batches:\n",
    "     - Faster Training: Larger batches can lead to faster training, as they utilize more parallelism and efficient hardware utilization.\n",
    "     - Regularization: Larger batches can act as implicit regularization, leading to smoother convergence and potentially better generalization.\n",
    "     - Better GPU Utilization: Larger batches can fully utilize GPU memory and computational resources.\n",
    "\n",
    "   - Drawbacks of Larger Batches:\n",
    "     - Slower Convergence: Large batches may require more iterations to converge to a good solution due to less frequent weight updates.\n",
    "     - Memory Requirements: Larger batches demand more memory, which may limit model size or batch size in GPU memory-limited environments.\n",
    "     - Reduced Generalization: In some cases, very large batches may lead to poorer generalization, as they may escape local minima less easily.\n",
    "\n",
    "7. Why should we avoid starting training with a high learning rate?\n",
    "   - Starting training with a high learning rate can lead to issues such as unstable training, overshooting, and poor convergence. The reasons to avoid it include:\n",
    "     - Exploding Gradients: High learning rates can cause gradients to become too large, resulting in parameter updates that overshoot the optimal values.\n",
    "     - Divergence: A high learning rate can lead to model divergence, where the loss function increases rather than decreases during training.\n",
    "     - Loss of Fine Details: It may cause the model to skip over fine details in the loss landscape, preventing it from finding a good local minimum.\n",
    "\n",
    "8. What are the pros of studying with a high rate of learning?\n",
    "   - Using a high learning rate can have advantages in specific scenarios, such as:\n",
    "     - Faster Convergence: High learning rates can speed up the initial convergence of the model, leading to quicker training.\n",
    "     - Escaping Local Minima: A high learning rate may help the model escape shallow local minima during optimization.\n",
    "     - Exploration: It encourages exploration of a wider range of parameter space, potentially finding better solutions.\n",
    "\n",
    "9. Why do we want to end the training with a low learning rate?\n",
    "   - Ending training with a low learning rate is a common practice known as learning rate annealing or learning rate scheduling. The reasons for doing this include:\n",
    "     - Fine-Tuning: A low learning rate helps the model fine-tune its parameters, making small and precise adjustments to achieve better performance.\n",
    "     - Stability: It ensures stable convergence near the end of training, preventing overshooting or divergence.\n",
    "     - Improved Generalization: Lower learning rates tend to lead to models with better generalization on validation and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af518dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
