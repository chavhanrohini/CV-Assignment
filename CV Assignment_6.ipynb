{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb112b9b",
   "metadata": {},
   "source": [
    "1. Difference between TRAINABLE and NON-TRAINABLE PARAMETERS:\n",
    "\n",
    "   - Trainable Parameters: These are the model parameters that are updated during the training process to minimize the loss function. They include weights and biases in neural network layers and are learned from the training data.\n",
    "   - Non-trainable Parameters: These are fixed model parameters that are not updated during training. They are typically used for tasks like feature extraction or predefined operations. Examples include the weights of a pre-trained model used for transfer learning or hyperparameters set in the architecture (e.g., filter size in a convolutional layer).\n",
    "\n",
    "2. In CNN architecture, where does the DROPOUT LAYER go?\n",
    "\n",
    "   - Dropout layers are typically placed after fully connected (dense) layers in a CNN. They are used to prevent overfitting by randomly deactivating a certain percentage of neurons during training. You can also place dropout layers after convolutional layers if needed, although it's more common after fully connected layers.\n",
    "\n",
    "3. What is the optimal number of hidden layers to stack?\n",
    "\n",
    "   - The optimal number of hidden layers in a neural network, including CNNs, varies depending on the specific task and dataset. There is no one-size-fits-all answer. It's common to start with a simple architecture and gradually increase the number of layers while monitoring performance on validation data. The optimal number of layers should balance model complexity and the risk of overfitting.\n",
    "\n",
    "4. In each layer, how many units or filters should there be?\n",
    "\n",
    "   - The number of units or filters in each layer is a hyperparameter that depends on the specific architecture and task. It is typically determined through experimentation. For convolutional layers in a CNN, the number of filters should increase with the depth of the network to capture more complex features. For fully connected layers, the number of units should be chosen based on the network's capacity and the problem's complexity.\n",
    "\n",
    "5. What should your initial learning rate be?\n",
    "\n",
    "   - The initial learning rate is a hyperparameter that depends on the optimization algorithm and the specific problem. A common starting point is to use a learning rate of 0.1 and then adjust it during training using learning rate schedules (e.g., learning rate annealing) or techniques like grid search or random search.\n",
    "\n",
    "6. What do you do with the activation function?\n",
    "\n",
    "   - Activation functions introduce non-linearity into the neural network, allowing it to learn complex relationships in the data. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh. You choose an activation function for each layer based on the problem and architecture requirements. ReLU is widely used as the default choice for hidden layers in many situations.\n",
    "\n",
    "7. What is NORMALIZATION OF DATA?\n",
    "\n",
    "   - Data normalization is a preprocessing step to scale and standardize the data. It helps the model converge faster and prevents numerical instability. Common methods include Z-score normalization (subtract mean and divide by standard deviation) or min-max scaling (scaling values to a specific range like [0, 1] or [-1, 1]).\n",
    "\n",
    "8. What is IMAGE AUGMENTATION and how does it work?\n",
    "\n",
    "   - Image augmentation is a technique used to artificially increase the diversity of the training dataset by applying various transformations to the images. This includes operations like rotation, flipping, scaling, cropping, and changes in brightness/contrast. Augmentation helps the model generalize better to unseen data by exposing it to a wider range of variations during training.\n",
    "\n",
    "9. What is DECLINE IN LEARNING RATE?\n",
    "\n",
    "   - A decline in learning rate refers to the practice of reducing the learning rate during the training process. It is often used to fine-tune the training process. Learning rate schedules, such as learning rate decay or annealing, decrease the learning rate over time, allowing the model to converge more effectively and reach a better minimum of the loss function.\n",
    "\n",
    "10. What does EARLY STOPPING CRITERIA mean?\n",
    "\n",
    "   - Early stopping is a technique in training deep learning models where the training is halted when a certain criterion is met. Typically, this criterion is based on the performance of the model on a validation dataset. The model's training is stopped when the validation performance stops improving or starts degrading. This prevents overfitting and allows you to use the model at the point where it performs best on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec53d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
